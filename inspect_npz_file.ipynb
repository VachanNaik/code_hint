{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['balm_vectors', 'pkd_scores']\n",
      "(1044, 128)\n",
      "[5.20387571 8.78923055 9.87197404 6.18613396 5.27380819]\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "  1.57921282  0.76743473 -0.46947439  0.54256004 -0.46341769 -0.46572975\n",
      "  0.24196227 -1.91328024 -1.72491783 -0.56228753 -1.01283112  0.31424733\n",
      " -0.90802408 -1.4123037   1.46564877 -0.2257763   0.0675282  -1.42474819\n",
      " -0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
      " -0.60170661  1.85227818 -0.01349722 -1.05771093  0.82254491 -1.22084365\n",
      "  0.2088636  -1.95967012 -1.32818605  0.19686124  0.73846658  0.17136828\n",
      " -0.11564828 -0.3011037  -1.47852199 -0.71984421 -0.46063877  1.05712223\n",
      "  0.34361829 -1.76304016  0.32408397 -0.38508228 -0.676922    0.61167629\n",
      "  1.03099952  0.93128012 -0.83921752 -0.30921238  0.33126343  0.97554513\n",
      " -0.47917424 -0.18565898 -1.10633497 -1.19620662  0.81252582  1.35624003\n",
      " -0.07201012  1.0035329   0.36163603 -0.64511975  0.36139561  1.53803657\n",
      " -0.03582604  1.56464366 -2.6197451   0.8219025   0.08704707 -0.29900735\n",
      "  0.09176078 -1.98756891 -0.21967189  0.35711257  1.47789404 -0.51827022\n",
      " -0.8084936  -0.50175704  0.91540212  0.32875111 -0.5297602   0.51326743\n",
      "  0.09707755  0.96864499 -0.70205309 -0.32766215 -0.39210815 -1.46351495\n",
      "  0.29612028  0.26105527  0.00511346 -0.23458713 -1.41537074 -0.42064532\n",
      " -0.34271452 -0.80227727 -0.16128571  0.40405086  1.8861859   0.17457781\n",
      "  0.25755039 -0.07444592 -1.91877122 -0.02651388  0.06023021  2.46324211\n",
      " -0.19236096  0.30154734 -0.03471177 -1.16867804  1.14282281  0.75193303\n",
      "  0.79103195 -0.90938745  1.40279431 -1.40185106  0.58685709  2.19045563\n",
      " -0.99053633 -0.56629773]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"/home/naikv12/Hint2/clinical-trial-outcome-prediction-main/data/target_protein/phase_I_train_balm.npz\")\n",
    "print(data.files)  # ['balm_vectors', 'pkd_scores']\n",
    "\n",
    "# Access arrays:\n",
    "balm_vecs = data['balm_vectors']\n",
    "pkd_scores = data['pkd_scores']\n",
    "\n",
    "print(balm_vecs.shape)     # (1044, 128)\n",
    "print(pkd_scores[:5])      # show first 5 pKd scores\n",
    "print(balm_vecs[0]) # â†’ vector for trial 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Index     Drug_ID                                               Drug  \\\n",
      "0          0    444607.0          Cc1ccc(CNS(=O)(=O)c2ccc(S(N)(=O)=O)s2)cc1   \n",
      "1          1      4316.0         COc1ccc(CNS(=O)(=O)c2ccc(S(N)(=O)=O)s2)cc1   \n",
      "2          2      4293.0              NS(=O)(=O)c1ccc(S(=O)(=O)NCc2cccs2)s1   \n",
      "3          3      1611.0       NS(=O)(=O)c1cc2c(s1)S(=O)(=O)N(Cc1cccs1)CC2O   \n",
      "4          4      1612.0     COc1ccc(N2CC(O)c3cc(S(N)(=O)=O)sc3S2(=O)=O)cc1   \n",
      "...      ...         ...                                                ...   \n",
      "24695  47936      3019.0                       CC1=Nc2ccc(Cl)cc2S(=O)(=O)N1   \n",
      "24696  47937  76311095.0  O=C(NCc1ccc(S(=O)(=O)N2CCN(C3COC3)CC2)cc1)c1cc...   \n",
      "24697  47938  76311094.0           O=C(NCCCCS(=O)(=O)c1ccccc1)c1ccc2nccn2c1   \n",
      "24698  47939    113557.0                        CCCCCCCOC1OC(CO)C(O)C(O)C1O   \n",
      "24699  47940    113557.0                        CCCCCCCOC1OC(CO)C(O)C(O)C1O   \n",
      "\n",
      "      Target_ID                                             Target         Y  \n",
      "0        P00918  MSHHWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKP...  9.251812  \n",
      "1        P00918  MSHHWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKP...  9.229148  \n",
      "2        P00918  MSHHWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKP...  9.031517  \n",
      "3        P00918  MSHHWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKP...  9.522879  \n",
      "4        P00918  MSHHWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKP...  9.585027  \n",
      "...         ...                                                ...       ...  \n",
      "24695    P43490  MNPAAEAEFNILLATDSYKVTHYKQYPPNTSKVYSYFECREKKTEN...  3.638272  \n",
      "24696    P43490  MNPAAEAEFNILLATDSYKVTHYKQYPPNTSKVYSYFECREKKTEN...  7.096367  \n",
      "24697    P43490  MNPAAEAEFNILLATDSYKVTHYKQYPPNTSKVYSYFECREKKTEN...  6.065451  \n",
      "24698    P08191  MKRVITLFAVLLMGWSVNAWSFACKTANGTAIPIGGGSANVYVNLA...  7.767004  \n",
      "24699    P08191  MKRVITLFAVLLMGWSVNAWSFACKTANGTAIPIGGGSANVYVNLA...  7.718967  \n",
      "\n",
      "[24700 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the BALM-Benchmark dataset (e.g., BindingDB_filtered)\n",
    "dataset = pd.read_csv(\"/home/naikv12/Hint2/clinical-trial-outcome-prediction-main/data/target_protein/data.csv\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Binding Affinity (pKd): -0.1926\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import selfies as sf\n",
    "\n",
    "# Load the pre-trained ChemFIE-DTP model\n",
    "model_name = \"gbyuvd/drugtargetpred-chemselfies\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a custom model with a regression head\n",
    "class RegressionHeadModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(RegressionHeadModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.regression_head = nn.Linear(base_model.config.hidden_size, 1)  # Output a single scalar value (pKd)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs[0]  # The first element of the output is the last hidden state\n",
    "        pooled_output = last_hidden_state[:, 0]  # Get the first token's representation (CLS token)\n",
    "        pKd = self.regression_head(pooled_output)  # Apply the regression head\n",
    "        return pKd\n",
    "\n",
    "# Initialize the model with a regression head\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "model_with_regression = RegressionHeadModel(base_model)\n",
    "\n",
    "# Move the model to the selected device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_with_regression.to(device)\n",
    "\n",
    "# Convert SMILES to SELFIES representation using the SELFIES library\n",
    "def smiles_to_selfies(smiles):\n",
    "    try:\n",
    "        selfies = sf.encoder(smiles)  # Encode SMILES into SELFIES\n",
    "        selfies_tokens = list(sf.split_selfies(selfies))\n",
    "        \n",
    "        # Join tokens appropriately\n",
    "        joined_tokens = []\n",
    "        i = 0\n",
    "        while i < len(selfies_tokens):\n",
    "            if selfies_tokens[i] == '.' and i + 1 < len(selfies_tokens):\n",
    "                joined_tokens.append(f\".{selfies_tokens[i+1]}\")\n",
    "                i += 2\n",
    "            else:\n",
    "                joined_tokens.append(selfies_tokens[i])\n",
    "                i += 1\n",
    "        \n",
    "        selfies_sentence = ' '.join(joined_tokens)\n",
    "        return selfies_sentence\n",
    "    except sf.EncoderError as e:\n",
    "        print(f\"Encoder Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example SMILES and target sequence\n",
    "smiles = \"CS[C@@H]1O[C@H](CO)[C@H](O)[C@H](O)[C@H]1O\"\n",
    "target_seq = \"MADNFSLHDALSGSGNPNPQGWPGAWGNQPAGAGGYPGASYPGAYPGQAPPGAYPGQAPPGAYPGAPGAYPGAPAPGVYPGPPSGPGAYPSSGQPSATGAYPATGPYGAPAGPLIVPYNLPLPGGVVPRMLITILGTVKPNANRIALDFQRGNDVAFHFNPRFNENNRRVIVCNTKLDNNWGREERQSVFPFESGKPFKIQVLVEPDHFKVAVNDAHLLQYNHRVKKLNEISKLGISGDIDLTSASYTMI\"\n",
    "\n",
    "# Convert the SMILES string to SELFIES representation\n",
    "selfies_representation = smiles_to_selfies(smiles)\n",
    "\n",
    "# Tokenize the inputs (SELFIES + target sequence)\n",
    "inputs = tokenizer(selfies_representation, target_seq, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Move the inputs to the device (GPU or CPU)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Get predicted binding affinity (pKd)\n",
    "with torch.no_grad():  # Disable gradients since we're only predicting\n",
    "    predicted_pKd = model_with_regression(**inputs)\n",
    "\n",
    "# Output the predicted binding affinity\n",
    "print(f\"Predicted Binding Affinity (pKd): {predicted_pKd.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Binding Affinity (pKd): 0.6084\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import selfies as sf\n",
    "\n",
    "# Load the pre-trained ChemFIE-DTP model\n",
    "model_name = \"gbyuvd/drugtargetpred-chemselfies\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a custom model with a regression head\n",
    "class RegressionHeadModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(RegressionHeadModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.regression_head = nn.Linear(base_model.config.hidden_size, 1)  # Output a single scalar value (pKd)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs[0]  # The first element of the output is the last hidden state\n",
    "        pooled_output = last_hidden_state[:, 0]  # Get the first token's representation (CLS token)\n",
    "        pKd = self.regression_head(pooled_output)  # Apply the regression head\n",
    "        return pKd\n",
    "\n",
    "# Initialize the model with a regression head\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "model_with_regression = RegressionHeadModel(base_model)\n",
    "\n",
    "# Move the model to the selected device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_with_regression.to(device)\n",
    "\n",
    "# Convert SMILES to SELFIES representation using the SELFIES library\n",
    "def smiles_to_selfies(smiles):\n",
    "    try:\n",
    "        selfies = sf.encoder(smiles)  # Encode SMILES into SELFIES\n",
    "        selfies_tokens = list(sf.split_selfies(selfies))\n",
    "        \n",
    "        # Join tokens appropriately\n",
    "        joined_tokens = []\n",
    "        i = 0\n",
    "        while i < len(selfies_tokens):\n",
    "            if selfies_tokens[i] == '.' and i + 1 < len(selfies_tokens):\n",
    "                joined_tokens.append(f\".{selfies_tokens[i+1]}\")\n",
    "                i += 2\n",
    "            else:\n",
    "                joined_tokens.append(selfies_tokens[i])\n",
    "                i += 1\n",
    "        \n",
    "        selfies_sentence = ' '.join(joined_tokens)\n",
    "        return selfies_sentence\n",
    "    except sf.EncoderError as e:\n",
    "        print(f\"Encoder Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example SMILES and target sequence\n",
    "smiles = \"NS(=O)(=O)c1ccc(S(=O)(=O)NCc2cccs2)s1\"\n",
    "target_seq = \"MSHHWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKPLSVSYDQATSLRILNNGHAFNVEFDDSQDKAVLKGGPLDGTYRLIQFHFHWGSLDGQGSEHTVDKKKYAAELHLVHWNTKYGDFGKAVQQPDGLAVLGIFLKVGSAKPGLQKVVDVLDSIKTKGKSADFTNFDPRGLLPESLDYWTYPGSLTTPPLLECVTWIVLKEPISVSSEQVLKFRKLNFNGEGEPEELMVDNWRPAQPLKNRQIKASFK\"\n",
    "\n",
    "# Convert the SMILES string to SELFIES representation\n",
    "selfies_representation = smiles_to_selfies(smiles)\n",
    "\n",
    "# Tokenize the inputs (SELFIES + target sequence)\n",
    "inputs = tokenizer(selfies_representation, target_seq, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Move the inputs to the device (GPU or CPU)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Get predicted binding affinity (pKd)\n",
    "with torch.no_grad():  # Disable gradients since we're only predicting\n",
    "    predicted_pKd = model_with_regression(**inputs)\n",
    "\n",
    "# Output the predicted binding affinity\n",
    "print(f\"Predicted Binding Affinity (pKd): {predicted_pKd.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_63', 'score': 0.006647514645010233}]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set the Hugging Face API token (replace with your token)\n",
    "api_token = \"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\"\n",
    "\n",
    "# Log in with the token\n",
    "login(token=api_token)\n",
    "# Load the model (e.g., DeepChem/ChemBERTa-77M-MTR)\n",
    "model_name = \"DeepChem/ChemBERTa-77M-MTR\"  # Replace with the actual model name\n",
    "model = pipeline(\"text-classification\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "# Example input (SMILES + target sequence)\n",
    "smiles = \"Cc1ccc(CNS(=O)(=O)c2ccc(S(N)(=O)=O)s2)cc1\"  # Example SMILES string\n",
    "target_seq = \"MSHHWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKPLSVSYDQATSLRILNNGHAFNVEFDDSQDKAVLKGGPLDGTYRLIQFHFHWGSLDGQGSEHTVDKKKYAAELHLVHWNTKYGDFGKAVQQPDGLAVLGIFLKVGSAKPGLQKVVDVLDSIKTKGKSADFTNFDPRGLLPESLDYWTYPGSLTTPPLLECVTWIVLKEPISVSSEQVLKFRKLNFNGEGEPEELMVDNWRPAQPLKNRQIKASFK\"  # Example target protein sequence\n",
    "\n",
    "# Get the prediction\n",
    "predictions = model(f\"{smiles} {target_seq}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7', 8: 'LABEL_8', 9: 'LABEL_9', 10: 'LABEL_10', 11: 'LABEL_11', 12: 'LABEL_12', 13: 'LABEL_13', 14: 'LABEL_14', 15: 'LABEL_15', 16: 'LABEL_16', 17: 'LABEL_17', 18: 'LABEL_18', 19: 'LABEL_19', 20: 'LABEL_20', 21: 'LABEL_21', 22: 'LABEL_22', 23: 'LABEL_23', 24: 'LABEL_24', 25: 'LABEL_25', 26: 'LABEL_26', 27: 'LABEL_27', 28: 'LABEL_28', 29: 'LABEL_29', 30: 'LABEL_30', 31: 'LABEL_31', 32: 'LABEL_32', 33: 'LABEL_33', 34: 'LABEL_34', 35: 'LABEL_35', 36: 'LABEL_36', 37: 'LABEL_37', 38: 'LABEL_38', 39: 'LABEL_39', 40: 'LABEL_40', 41: 'LABEL_41', 42: 'LABEL_42', 43: 'LABEL_43', 44: 'LABEL_44', 45: 'LABEL_45', 46: 'LABEL_46', 47: 'LABEL_47', 48: 'LABEL_48', 49: 'LABEL_49', 50: 'LABEL_50', 51: 'LABEL_51', 52: 'LABEL_52', 53: 'LABEL_53', 54: 'LABEL_54', 55: 'LABEL_55', 56: 'LABEL_56', 57: 'LABEL_57', 58: 'LABEL_58', 59: 'LABEL_59', 60: 'LABEL_60', 61: 'LABEL_61', 62: 'LABEL_62', 63: 'LABEL_63', 64: 'LABEL_64', 65: 'LABEL_65', 66: 'LABEL_66', 67: 'LABEL_67', 68: 'LABEL_68', 69: 'LABEL_69', 70: 'LABEL_70', 71: 'LABEL_71', 72: 'LABEL_72', 73: 'LABEL_73', 74: 'LABEL_74', 75: 'LABEL_75', 76: 'LABEL_76', 77: 'LABEL_77', 78: 'LABEL_78', 79: 'LABEL_79', 80: 'LABEL_80', 81: 'LABEL_81', 82: 'LABEL_82', 83: 'LABEL_83', 84: 'LABEL_84', 85: 'LABEL_85', 86: 'LABEL_86', 87: 'LABEL_87', 88: 'LABEL_88', 89: 'LABEL_89', 90: 'LABEL_90', 91: 'LABEL_91', 92: 'LABEL_92', 93: 'LABEL_93', 94: 'LABEL_94', 95: 'LABEL_95', 96: 'LABEL_96', 97: 'LABEL_97', 98: 'LABEL_98', 99: 'LABEL_99', 100: 'LABEL_100', 101: 'LABEL_101', 102: 'LABEL_102', 103: 'LABEL_103', 104: 'LABEL_104', 105: 'LABEL_105', 106: 'LABEL_106', 107: 'LABEL_107', 108: 'LABEL_108', 109: 'LABEL_109', 110: 'LABEL_110', 111: 'LABEL_111', 112: 'LABEL_112', 113: 'LABEL_113', 114: 'LABEL_114', 115: 'LABEL_115', 116: 'LABEL_116', 117: 'LABEL_117', 118: 'LABEL_118', 119: 'LABEL_119', 120: 'LABEL_120', 121: 'LABEL_121', 122: 'LABEL_122', 123: 'LABEL_123', 124: 'LABEL_124', 125: 'LABEL_125', 126: 'LABEL_126', 127: 'LABEL_127', 128: 'LABEL_128', 129: 'LABEL_129', 130: 'LABEL_130', 131: 'LABEL_131', 132: 'LABEL_132', 133: 'LABEL_133', 134: 'LABEL_134', 135: 'LABEL_135', 136: 'LABEL_136', 137: 'LABEL_137', 138: 'LABEL_138', 139: 'LABEL_139', 140: 'LABEL_140', 141: 'LABEL_141', 142: 'LABEL_142', 143: 'LABEL_143', 144: 'LABEL_144', 145: 'LABEL_145', 146: 'LABEL_146', 147: 'LABEL_147', 148: 'LABEL_148', 149: 'LABEL_149', 150: 'LABEL_150', 151: 'LABEL_151', 152: 'LABEL_152', 153: 'LABEL_153', 154: 'LABEL_154', 155: 'LABEL_155', 156: 'LABEL_156', 157: 'LABEL_157', 158: 'LABEL_158', 159: 'LABEL_159', 160: 'LABEL_160', 161: 'LABEL_161', 162: 'LABEL_162', 163: 'LABEL_163', 164: 'LABEL_164', 165: 'LABEL_165', 166: 'LABEL_166', 167: 'LABEL_167', 168: 'LABEL_168', 169: 'LABEL_169', 170: 'LABEL_170', 171: 'LABEL_171', 172: 'LABEL_172', 173: 'LABEL_173', 174: 'LABEL_174', 175: 'LABEL_175', 176: 'LABEL_176', 177: 'LABEL_177', 178: 'LABEL_178', 179: 'LABEL_179', 180: 'LABEL_180', 181: 'LABEL_181', 182: 'LABEL_182', 183: 'LABEL_183', 184: 'LABEL_184', 185: 'LABEL_185', 186: 'LABEL_186', 187: 'LABEL_187', 188: 'LABEL_188', 189: 'LABEL_189', 190: 'LABEL_190', 191: 'LABEL_191', 192: 'LABEL_192', 193: 'LABEL_193', 194: 'LABEL_194', 195: 'LABEL_195', 196: 'LABEL_196', 197: 'LABEL_197', 198: 'LABEL_198'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the model configuration\n",
    "model_name = \"DeepChem/ChemBERTa-77M-MTR\"  # Replace with the actual model name\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Check the model's config to see if there is a label mapping\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the amino acids\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "# Function to one-hot encode a sequence\n",
    "def one_hot_encode(sequence):\n",
    "    encoding = []\n",
    "    for aa in sequence:\n",
    "        encoding.append([1 if aa == x else 0 for x in amino_acids])\n",
    "    return np.array(encoding)\n",
    "\n",
    "# Example protein sequence\n",
    "protein_sequence = \"MNEKGT\"\n",
    "encoded_sequence = one_hot_encode(protein_sequence)\n",
    "print(encoded_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
